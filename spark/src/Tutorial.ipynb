{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d7e8c87-a7a2-41bc-a013-a30f8fd16daa",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "Spark's Python wrapper lets us interact with data very similarly to Pandas, which should be very familiar to Python users. In this notebook you will learn how to use the basic functionality of the wrapper, as well as visualize the data that you will be working with for the project. Make sure you have downloaded and unzipped the data to the correct location before trying to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de7de5d-9874-4311-b671-6fa09ba2b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd \n",
    "import numpy\n",
    "import matplotlib.pyplot as plt \n",
    "from pyspark.sql import SparkSession, dataframe\n",
    "import plotly.express as px\n",
    "geojson = px.data.gapminder()\n",
    "# create sparksession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CS236\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeff980-1c7e-471e-b4fd-9ff2a852c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to write query plans to a file\n",
    "# you will be using this to understand how your queries are being processed\n",
    "def write_explain(df: dataframe.DataFrame, output_path: str = \"out.txt\"):\n",
    "    from contextlib import redirect_stdout\n",
    "    with open(output_path, \"w\") as f:\n",
    "        with redirect_stdout(f):\n",
    "            df.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8b15d-3dca-4f90-b635-c3af2eebc701",
   "metadata": {},
   "source": [
    "Read a csv to a Spark dataframe, then return the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b3171e-95dd-41d2-a216-3ef2399e1034",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "sdf = spark.read.csv(\"../data/StateAndCountyData.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e36b5b-4598-4a1e-858d-3b97f3f46753",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.csv(\"../data/StateAndCountyData.csv\", header=True)\n",
    "sdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f995d4f8-d7cb-43d6-a3d3-b216aa204eb2",
   "metadata": {},
   "source": [
    "Show the first 20 rows of the Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bd3b8a-0528-4514-bf53-c8727e5f4a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbdd528-0da9-40bd-97eb-d40649795d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.createOrReplaceTempView('state_county')\n",
    "# run your SQL query as you would with any database\n",
    "my_df = spark.sql(\n",
    "'''\n",
    "select \n",
    "  state\n",
    "  , avg(value) as avg\n",
    "from state_county\n",
    "where variable_code = 'PCT_LACCESS_POP15' \n",
    "group by state\n",
    "order by state\n",
    "'''\n",
    ")\n",
    "my_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1bf510-8c6c-4b44-8242-f24b583d15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_explain(my_df)\n",
    "# print out the query plan\n",
    "my_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f7b2fe-9844-4929-bbea-bfa5fd7af0aa",
   "metadata": {},
   "source": [
    "## Visualizing with Choropleths\n",
    "We will be using Plotly Express to easily visualize the data you will be working with. The most important arguments besides the dataframe itself are `locations` and `color`.\n",
    "- `locations` - the name of the column that defines which values go into which state in the chart\n",
    "- `color` - the name of the column that contains the values to be displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feaa4c7-5e22-48a7-813e-5fc7bffac512",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(my_df,\n",
    "                    locations='state',\n",
    "                    color='avg',\n",
    "                    color_continuous_scale='spectral_r',\n",
    "                    locationmode='USA-states',\n",
    "                    scope='usa')\n",
    "fig.update_geos(\n",
    "    visible=True, \n",
    "    scope=\"usa\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452436c8-808e-4a66-98cc-395ae5614d91",
   "metadata": {},
   "source": [
    "# Reading into RDD's\n",
    "RDD (Resilient Distributed Dataset) allows you to have a finer control over the parallelization of your jobs. \n",
    "The thought process is very similar to Hadoop, but with less boilerplate.\n",
    "\n",
    "Note that there are many (probably better) ways to do the same task in Spark. Experiment with different methods when doing your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2e9bc-cf8b-4b95-8802-0a3c70648da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.sparkContext.textFile(\"../data/StateAndCountyData.csv\")\n",
    "header = data.first()\n",
    "# Remove the header from the file. It is not enough to just remove the first row,\n",
    "# since some spark applications take in multiple files each with headers.\n",
    "data = data.filter(lambda row: row != header)\n",
    "data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91475875-a7dc-498b-b410-29595d8ed38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column names to make it easier to reference column values later on\n",
    "cols_dict = dict([(j, i) for i, j in enumerate(header.split(\",\"))])\n",
    "cols_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ec01ef-a078-46a1-af1b-5e92bc0841b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split lines into column values\n",
    "cols_rdd = data.map(lambda row: row.split(\",\"))\n",
    "cols_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d90d2ae-a2f0-43dc-a2bb-8656d17fcf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter values to the variable code we want\n",
    "pct_laccess_pop_15_rdd = cols_rdd.filter(lambda row: row[cols_dict[\"Variable_Code\"]] == \"PCT_LACCESS_POP15\")\n",
    "pct_laccess_pop_15_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c9b10-bd30-411e-b789-5f5c6eb67cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map states to their respective values\n",
    "states_values_rdd = pct_laccess_pop_15_rdd.map(\n",
    "    lambda row: (row[cols_dict[\"State\"]], float(row[cols_dict[\"Value\"]]))\n",
    ")\n",
    "states_values_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253cb13d-476a-4aa6-8a01-d97f5838b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total values & sums for each state\n",
    "reduce_rdd = states_values_rdd.aggregateByKey(\n",
    "    # starting (sum, count) values for each state\n",
    "    (0, 0),\n",
    "    # accumulates the (sum, count) tuple to calculate the average\n",
    "    # this one runs between value (think rows)\n",
    "    lambda accum, value: (accum[0] + value, accum[1] + 1), \n",
    "    # accumulates the (sum, count) tuples between the partitions\n",
    "    lambda accum_1, accum_2: (accum_1[0] + accum_2[0], accum_1[1] + accum_2[1]))\n",
    "reduce_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933b463-a540-4d8c-a29a-df4f8116eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average by doing sum / count\n",
    "average_rdd = aggregate_rdd.mapValues(lambda accum: accum[0] / accum[1])\n",
    "cols = [\"state\", \"avg\"]\n",
    "# convert to a Pandas dataframe to visualize\n",
    "df = pd.DataFrame(average_rdd.collect(), columns=cols)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75e889-e3af-4bfa-b91d-3d1330dae0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(df,\n",
    "                    locations='state',\n",
    "                    color='avg',\n",
    "                    color_continuous_scale='spectral_r',\n",
    "                    locationmode='USA-states',\n",
    "                    scope='usa')\n",
    "fig.update_geos(\n",
    "    visible=True, \n",
    "    scope=\"usa\",\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
